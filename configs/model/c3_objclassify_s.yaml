#                 from  n    params  module                                  arguments
#  0                -1       880  models.layers.common_layer.Conv         [3, 8, 6, 2, 2]
#  1                -1      1184  models.layers.common_layer.Conv         [8, 16, 3, 2]
#  2                -1      1248  models.layers.yolo_layer.C3             [16, 16, 1]
#  3                -1      4672  models.layers.common_layer.Conv         [16, 32, 3, 2]
#  4                -1      7424  models.layers.yolo_layer.C3             [32, 32, 2]
#  5                -1     18560  models.layers.common_layer.Conv         [32, 64, 3, 2]
#  6                -1     39552  models.layers.yolo_layer.C3             [64, 64, 3]
#  7                -1     73984  models.layers.common_layer.Conv         [64, 128, 3, 2]
#  8                -1       129  models.head.Classify                    [128, 1]
#Model Summary: 122layers, 147633 parameters, 147633 gradients,image size is 640, 0.7 GFLOPs in cpu
# time (ms)     GFLOPs     params                                    module                       input_size
#     12.70     1.4680        880  models.layers.common_layer.Conv           torch.Size([8, 3, 640, 640])
#      2.40     0.4981       1184  models.layers.common_layer.Conv           torch.Size([8, 8, 320, 320])
#      8.10     0.5505       1248  models.layers.yolo_layer.C3               torch.Size([8, 16, 160, 160])
#      1.20     0.4850       4672  models.layers.common_layer.Conv           torch.Size([8, 16, 160, 160])
#      1.30     0.7864       7424  models.layers.yolo_layer.C3               torch.Size([8, 32, 80, 80])
#      0.20     0.4784      18560  models.layers.common_layer.Conv           torch.Size([8, 32, 80, 80])
#      1.90     1.0289      39552  models.layers.yolo_layer.C3               torch.Size([8, 64, 40, 40])
#      0.20     0.4751      73984  models.layers.common_layer.Conv           torch.Size([8, 64, 40, 40])
#      0.20     0.0008        129  models.head.Classify                      torch.Size([8, 128, 20, 20])

# Parameters
depth_multiple: 0.33  # model depth multiple
width_multiple: 0.25 # layer channel multiple
depth_layer: [C3]
width_layer: [Conv, C3, SPPF, ObjClassify]
nc: 80

backbone:
  [[-1, Conv, [32, 6, 2, 2]],  # 0-P1/2
   [-1, Conv, [32, 64, 3, 2]],  # 1-P2/4
   [-1, C3, [64, 64, 3]],
   [-1, Conv, [64, 128, 3, 2]],  # 3-P3/8
   [-1, C3, [128, 128, 6]],
   [-1, Conv, [128, 256, 3, 2]],  # 5-P4/16
   [-1, C3, [256, 256, 9]],
   [-1, Conv, [256, 512, 3, 2]], # 7-P5/32
  ]

neck:
  [[-1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 6], Concat, [1]],  # cat backbone P4

   [-1, Conv, [768, 256, 3, 1]],
   [-1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 4], Concat, [1]],  # cat backbone P3

   [-1, Conv, [384, 128, 3, 1]],
   [-1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 2], Concat, [1]],  # cat backbone P3
  ]
head:
  [
    #[[17, 20, 23], Classify, [1024, 2]],  # Detect(P3, P4, P5)
    [-1, ObjClassify, [192, nc]],  # Detect(P3, P4, P5)
  ]

